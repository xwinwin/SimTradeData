"""
åŒæ­¥ç®¡ç†å™¨

ç»Ÿä¸€ç®¡ç†å¢é‡åŒæ­¥ã€ç¼ºå£æ£€æµ‹å’Œæ•°æ®éªŒè¯åŠŸèƒ½ã€‚
"""

# æ ‡å‡†åº“å¯¼å…¥
import logging
from datetime import date, datetime, timedelta
from typing import Any, Dict, List

# é¡¹ç›®å†…å¯¼å…¥
from ..config import Config
from ..core import BaseManager, ValidationError, unified_error_handler
from ..data_sources import DataSourceManager
from ..database import DatabaseManager
from ..preprocessor import DataProcessingEngine
from ..utils.progress_bar import (
    create_phase_progress,
    log_error,
    log_phase_complete,
    log_phase_start,
    update_phase_description,
)
from .gap_detector import GapDetector
from .incremental import IncrementalSync
from .validator import DataValidator

logger = logging.getLogger(__name__)


class SyncManager(BaseManager):
    """åŒæ­¥ç®¡ç†å™¨"""

    def __init__(
        self,
        db_manager: DatabaseManager,
        data_source_manager: DataSourceManager,
        processing_engine: DataProcessingEngine,
        config: Config = None,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            data_source_manager: æ•°æ®æºç®¡ç†å™¨
            processing_engine: æ•°æ®å¤„ç†å¼•æ“
            config: é…ç½®å¯¹è±¡
        """
        super().__init__(
            config=config,
            db_manager=db_manager,
            data_source_manager=data_source_manager,
            processing_engine=processing_engine,
            **kwargs,
        )

    def _init_specific_config(self):
        """åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨ç‰¹å®šé…ç½®"""
        self.enable_auto_gap_fix = self._get_config("sync_manager.auto_gap_fix", True)
        self.enable_validation = self._get_config(
            "sync_manager.enable_validation", True
        )
        self.max_gap_fix_days = self._get_config("sync_manager.max_gap_fix_days", 7)

    def _init_components(self):
        """åˆå§‹åŒ–å­ç»„ä»¶"""
        # åˆå§‹åŒ–å­ç»„ä»¶
        self.incremental_sync = IncrementalSync(
            self.db_manager,
            self.data_source_manager,
            self.processing_engine,
            self.config,
        )
        self.gap_detector = GapDetector(self.db_manager, self.config)
        self.validator = DataValidator(self.db_manager, self.config)

    def _get_required_attributes(self) -> List[str]:
        """å¿…éœ€å±æ€§åˆ—è¡¨"""
        return [
            "db_manager",
            "data_source_manager",
            "processing_engine",
            "incremental_sync",
            "gap_detector",
            "validator",
        ]

    @unified_error_handler(return_dict=True)
    def run_full_sync(
        self,
        target_date: date = None,
        symbols: List[str] = None,
        frequencies: List[str] = None,
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åŒæ­¥æµç¨‹

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ´»è·ƒè‚¡ç¥¨
            frequencies: é¢‘ç‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºé…ç½®ä¸­çš„é¢‘ç‡

        Returns:
            Dict[str, Any]: å®Œæ•´åŒæ­¥ç»“æœ
        """
        if not target_date:
            raise ValidationError("ç›®æ ‡æ—¥æœŸä¸èƒ½ä¸ºç©º")

        if target_date is None:
            target_date = datetime.now().date()

        # é™åˆ¶ç›®æ ‡æ—¥æœŸä¸èƒ½è¶…è¿‡ä»Šå¤©ï¼Œä½¿ç”¨åˆç†çš„å†å²æ—¥æœŸ
        today = datetime.now().date()
        if target_date > today:
            # å¦‚æœç›®æ ‡æ—¥æœŸæ˜¯æœªæ¥ï¼Œä½¿ç”¨æœ€è¿‘çš„äº¤æ˜“æ—¥
            target_date = date(2025, 1, 24)  # ä½¿ç”¨å·²çŸ¥æœ‰æ•°æ®çš„æ—¥æœŸ
            self._log_warning("run_full_sync", f"ç›®æ ‡æ—¥æœŸè°ƒæ•´ä¸ºå†å²æ—¥æœŸ: {target_date}")

        try:
            self._log_method_start("run_full_sync", target_date=target_date)
            start_time = datetime.now()

            full_result = {
                "target_date": str(target_date),
                "start_time": start_time.isoformat(),
                "phases": {},
                "summary": {
                    "total_phases": 0,
                    "successful_phases": 0,
                    "failed_phases": 0,
                },
            }

            # é˜¶æ®µ0: æ›´æ–°åŸºç¡€æ•°æ®ï¼ˆäº¤æ˜“æ—¥å†å’Œè‚¡ç¥¨åˆ—è¡¨ï¼‰
            log_phase_start("é˜¶æ®µ0", "æ›´æ–°åŸºç¡€æ•°æ®")

            with create_phase_progress("phase0", 2, "åŸºç¡€æ•°æ®æ›´æ–°", "é¡¹") as pbar:
                try:
                    # æ›´æ–°äº¤æ˜“æ—¥å†
                    update_phase_description("æ›´æ–°äº¤æ˜“æ—¥å†")
                    calendar_result = self._update_trading_calendar(target_date)
                    full_result["phases"]["calendar_update"] = calendar_result
                    full_result["summary"]["total_phases"] += 1
                    pbar.update(1)

                    if "error" not in calendar_result:
                        full_result["summary"]["successful_phases"] += 1
                        updated_records = calendar_result.get("updated_records", 0)
                        total_records = calendar_result.get("total_records", 0)
                        years_range = f"{calendar_result.get('start_year')}-{calendar_result.get('end_year')}"
                        log_phase_complete(
                            "äº¤æ˜“æ—¥å†æ›´æ–°",
                            {
                                "å¹´ä»½èŒƒå›´": years_range,
                                "æ–°å¢è®°å½•": f"{updated_records}æ¡",
                                "æ€»è®°å½•": f"{total_records}æ¡",
                            },
                        )
                    else:
                        full_result["summary"]["failed_phases"] += 1
                        log_error(f"äº¤æ˜“æ—¥å†æ›´æ–°å¤±è´¥: {calendar_result['error']}")

                    # æ›´æ–°è‚¡ç¥¨åˆ—è¡¨
                    update_phase_description("æ›´æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰")
                    stock_list_result = self._update_stock_list()
                    full_result["phases"]["stock_list_update"] = stock_list_result
                    full_result["summary"]["total_phases"] += 1
                    pbar.update(1)

                    if "error" not in stock_list_result:
                        full_result["summary"]["successful_phases"] += 1
                        total_stocks = stock_list_result.get("total_stocks", 0)
                        new_stocks = stock_list_result.get("new_stocks", 0)
                        updated_stocks = stock_list_result.get("updated_stocks", 0)
                        log_phase_complete(
                            "è‚¡ç¥¨åˆ—è¡¨æ›´æ–°",
                            {
                                "æ€»è‚¡ç¥¨": f"{total_stocks}åª",
                                "æ–°å¢": f"{new_stocks}åª",
                                "æ›´æ–°": f"{updated_stocks}åª",
                            },
                        )
                    else:
                        full_result["summary"]["failed_phases"] += 1
                        log_error(f"è‚¡ç¥¨åˆ—è¡¨æ›´æ–°å¤±è´¥: {stock_list_result['error']}")

                except Exception as e:
                    log_error(f"åŸºç¡€æ•°æ®æ›´æ–°å¤±è´¥: {e}")
                    full_result["phases"]["base_data_update"] = {"error": str(e)}
                    full_result["summary"]["total_phases"] += 1
                    full_result["summary"]["failed_phases"] += 1

            # å¦‚æœæ²¡æœ‰æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨
            if not symbols:
                symbols = self._get_active_stocks_from_db()
                if not symbols:
                    # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰è‚¡ç¥¨ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨
                    symbols = ["000001.SZ", "000002.SZ", "600000.SS", "600036.SS"]
                    self.logger.info(f"ä½¿ç”¨é»˜è®¤è‚¡ç¥¨åˆ—è¡¨: {len(symbols)}åªè‚¡ç¥¨")
                else:
                    self.logger.info(f"ä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨: {len(symbols)}åªè‚¡ç¥¨")

            # é˜¶æ®µ1: å¢é‡åŒæ­¥ï¼ˆå¸‚åœºæ•°æ®ï¼‰
            log_phase_start("é˜¶æ®µ1", "å¢é‡åŒæ­¥å¸‚åœºæ•°æ®")

            with create_phase_progress(
                "phase1", len(symbols), "å¢é‡åŒæ­¥", "è‚¡ç¥¨"
            ) as pbar:
                try:
                    # ä¿®æ”¹å¢é‡åŒæ­¥ä»¥æ”¯æŒè¿›åº¦å›è°ƒ
                    sync_result = self.incremental_sync.sync_all_symbols(
                        target_date, symbols, frequencies, progress_bar=pbar
                    )
                    full_result["phases"]["incremental_sync"] = {
                        "status": "completed",
                        "result": sync_result,
                    }
                    full_result["summary"]["successful_phases"] += 1

                    # ä»ç»“æœä¸­æå–ç»Ÿè®¡ä¿¡æ¯
                    success_count = sync_result.get("success_count", len(symbols))
                    error_count = sync_result.get("error_count", 0)
                    log_phase_complete(
                        "å¢é‡åŒæ­¥",
                        {"æˆåŠŸ": f"{success_count}åªè‚¡ç¥¨", "å¤±è´¥": error_count},
                    )

                except Exception as e:
                    log_error(f"å¢é‡åŒæ­¥å¤±è´¥: {e}")
                    full_result["phases"]["incremental_sync"] = {
                        "status": "failed",
                        "error": str(e),
                    }
                    full_result["summary"]["failed_phases"] += 1

            full_result["summary"]["total_phases"] += 1

            # é˜¶æ®µ2: åŒæ­¥æ‰©å±•æ•°æ®
            log_phase_start("é˜¶æ®µ2", "åŒæ­¥æ‰©å±•æ•°æ®")

            # é¢„æ£€æŸ¥æ‰©å±•æ•°æ®åŒæ­¥çš„æ–­ç‚¹ç»­ä¼ çŠ¶æ€
            extended_symbols_to_process = self._get_extended_data_symbols_to_process(
                symbols, target_date
            )

            self.logger.info(
                f"ğŸ“Š æ‰©å±•æ•°æ®åŒæ­¥: æ€»è‚¡ç¥¨ {len(symbols)}åª, éœ€å¤„ç† {len(extended_symbols_to_process)}åª"
            )

            # å¦‚æœæ²¡æœ‰è‚¡ç¥¨éœ€è¦å¤„ç†ï¼Œç›´æ¥è·³è¿‡
            if len(extended_symbols_to_process) == 0:
                self.logger.info("âœ… æ‰€æœ‰è‚¡ç¥¨çš„æ‰©å±•æ•°æ®å·²å®Œæˆï¼Œè·³è¿‡æ‰©å±•æ•°æ®åŒæ­¥")
                full_result["phases"]["extended_data_sync"] = {
                    "status": "skipped",
                    "result": {"message": "æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€å¤„ç†"},
                }
                full_result["summary"]["successful_phases"] += 1
                log_phase_complete("æ‰©å±•æ•°æ®åŒæ­¥", {"çŠ¶æ€": "å·²å®Œæˆï¼Œè·³è¿‡"})
            else:
                # ä½¿ç”¨éœ€è¦å¤„ç†çš„è‚¡ç¥¨æ•°é‡ä½œä¸ºè¿›åº¦æ¡åŸºå‡†
                with create_phase_progress(
                    "phase2", len(extended_symbols_to_process), "æ‰©å±•æ•°æ®åŒæ­¥", "è‚¡ç¥¨"
                ) as pbar:
                    try:
                        extended_result = self._sync_extended_data(
                            extended_symbols_to_process,
                            target_date,
                            pbar,  # åªä¼ å…¥éœ€è¦å¤„ç†çš„è‚¡ç¥¨
                        )
                        full_result["phases"]["extended_data_sync"] = {
                            "status": "completed",
                            "result": extended_result,
                        }
                        full_result["summary"]["successful_phases"] += 1

                        log_phase_complete(
                            "æ‰©å±•æ•°æ®åŒæ­¥",
                            {
                                "è´¢åŠ¡æ•°æ®": f"{extended_result.get('financials_count', 0)}æ¡",
                                "ä¼°å€¼æ•°æ®": f"{extended_result.get('valuations_count', 0)}æ¡",
                                "æŠ€æœ¯æŒ‡æ ‡": f"{extended_result.get('indicators_count', 0)}æ¡",
                            },
                        )

                    except Exception as e:
                        log_error(f"æ‰©å±•æ•°æ®åŒæ­¥å¤±è´¥: {e}")
                        full_result["phases"]["extended_data_sync"] = {
                            "status": "failed",
                            "error": str(e),
                        }
                        full_result["summary"]["failed_phases"] += 1

            full_result["summary"]["total_phases"] += 1

            # é˜¶æ®µ3: ç¼ºå£æ£€æµ‹
            log_phase_start("é˜¶æ®µ3", "ç¼ºå£æ£€æµ‹ä¸ä¿®å¤")

            with create_phase_progress(
                "phase2", len(symbols), "ç¼ºå£æ£€æµ‹", "è‚¡ç¥¨"
            ) as pbar:
                try:
                    gap_start_date = target_date - timedelta(days=30)  # æ£€æµ‹æœ€è¿‘30å¤©
                    gap_result = self.gap_detector.detect_all_gaps(
                        gap_start_date, target_date, symbols, frequencies
                    )

                    # æ›´æ–°è¿›åº¦
                    pbar.update(len(symbols))

                    full_result["phases"]["gap_detection"] = {
                        "status": "completed",
                        "result": gap_result,
                    }
                    full_result["summary"]["successful_phases"] += 1

                    total_gaps = gap_result["summary"]["total_gaps"]

                    # è‡ªåŠ¨ä¿®å¤ç¼ºå£
                    if self.enable_auto_gap_fix and total_gaps > 0:
                        update_phase_description(f"ä¿®å¤{total_gaps}ä¸ªç¼ºå£")
                        fix_result = self._auto_fix_gaps(gap_result)
                        full_result["phases"]["gap_fix"] = {
                            "status": "completed",
                            "result": fix_result,
                        }
                        log_phase_complete(
                            "ç¼ºå£æ£€æµ‹ä¸ä¿®å¤",
                            {"æ£€æµ‹": f"{total_gaps}ä¸ªç¼ºå£", "ä¿®å¤": "å®Œæˆ"},
                        )
                    else:
                        log_phase_complete("ç¼ºå£æ£€æµ‹", {"ç¼ºå£": f"{total_gaps}ä¸ª"})

                except Exception as e:
                    log_error(f"ç¼ºå£æ£€æµ‹å¤±è´¥: {e}")
                    full_result["phases"]["gap_detection"] = {
                        "status": "failed",
                        "error": str(e),
                    }
                    full_result["summary"]["failed_phases"] += 1

            full_result["summary"]["total_phases"] += 1

            # é˜¶æ®µ3: æ•°æ®éªŒè¯
            if self.enable_validation:
                log_phase_start("é˜¶æ®µ3", "æ•°æ®éªŒè¯")

                with create_phase_progress(
                    "phase3", len(symbols), "æ•°æ®éªŒè¯", "è‚¡ç¥¨"
                ) as pbar:
                    try:
                        validation_start_date = target_date - timedelta(
                            days=7
                        )  # éªŒè¯æœ€è¿‘7å¤©
                        validation_result = self.validator.validate_all_data(
                            validation_start_date, target_date, symbols, frequencies
                        )

                        # æ›´æ–°è¿›åº¦
                        pbar.update(len(symbols))

                        full_result["phases"]["validation"] = {
                            "status": "completed",
                            "result": validation_result,
                        }
                        full_result["summary"]["successful_phases"] += 1

                        # æå–éªŒè¯ç»Ÿè®¡
                        total_records = validation_result.get("total_records", 0)
                        valid_records = validation_result.get("valid_records", 0)
                        validation_rate = validation_result.get("validation_rate", 0)

                        log_phase_complete(
                            "æ•°æ®éªŒè¯",
                            {
                                "è®°å½•": f"{total_records}æ¡",
                                "æœ‰æ•ˆ": f"{valid_records}æ¡",
                                "éªŒè¯ç‡": f"{validation_rate:.1f}%",
                            },
                        )

                    except Exception as e:
                        log_error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
                        full_result["phases"]["validation"] = {
                            "status": "failed",
                            "error": str(e),
                        }
                        full_result["summary"]["failed_phases"] += 1

                full_result["summary"]["total_phases"] += 1

            # å®Œæˆæ—¶é—´
            end_time = datetime.now()
            full_result["end_time"] = end_time.isoformat()
            full_result["duration_seconds"] = (end_time - start_time).total_seconds()

            self._log_performance(
                "run_full_sync",
                full_result["duration_seconds"],
                successful_phases=full_result["summary"]["successful_phases"],
                failed_phases=full_result["summary"]["failed_phases"],
            )

            return full_result

        except Exception as e:
            self._log_error("run_full_sync", e, target_date=target_date)
            raise

    @unified_error_handler(return_dict=True)
    def get_sync_status(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥çŠ¶æ€"""
        try:
            # è·å–æœ€è¿‘çš„åŒæ­¥çŠ¶æ€
            sql = """
            SELECT * FROM sync_status
            ORDER BY last_sync_date DESC
            LIMIT 10
            """

            recent_syncs = self.db_manager.fetchall(sql)

            # è·å–æ•°æ®ç»Ÿè®¡
            stats_sql = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT symbol) as total_symbols,
                COUNT(DISTINCT date) as total_dates,
                MIN(date) as earliest_date,
                MAX(date) as latest_date,
                AVG(quality_score) as avg_quality
            FROM market_data
            """

            stats_result = self.db_manager.fetchone(stats_sql)

            return {
                "recent_syncs": [dict(row) for row in recent_syncs],
                "data_stats": dict(stats_result) if stats_result else {},
                "components": {
                    "incremental_sync": (
                        self.incremental_sync.get_sync_stats()
                        if hasattr(self.incremental_sync, "get_sync_stats")
                        else {}
                    ),
                    "gap_detector": {
                        "max_gap_days": getattr(self.gap_detector, "max_gap_days", 30),
                        "min_data_quality": getattr(
                            self.gap_detector, "min_data_quality", 0.8
                        ),
                    },
                    "validator": {
                        "min_data_quality": getattr(
                            self.validator, "min_data_quality", 0.8
                        ),
                        "max_price_change_pct": getattr(
                            self.validator, "max_price_change_pct", 20.0
                        ),
                    },
                },
                "config": {
                    "enable_auto_gap_fix": self.enable_auto_gap_fix,
                    "enable_validation": self.enable_validation,
                    "max_gap_fix_days": self.max_gap_fix_days,
                },
            }

        except Exception as e:
            self._log_error("get_sync_status", e)
            raise

    def _get_active_stocks_from_db(self) -> List[str]:
        """ä»æ•°æ®åº“è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨"""
        try:
            sql = "SELECT symbol FROM stocks WHERE status = 'active' ORDER BY symbol"
            result = self.db_manager.fetchall(sql)
            return [row["symbol"] for row in result] if result else []
        except Exception as e:
            self._log_warning(
                "_get_active_stocks_from_db", f"ä»æ•°æ®åº“è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}"
            )
            return []

    def _get_extended_data_symbols_to_process(
        self, symbols: List[str], target_date: date
    ) -> List[str]:
        """
        è·å–éœ€è¦å¤„ç†æ‰©å±•æ•°æ®çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆåŸºäºå®é™…æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å’Œæ–­ç‚¹ç»­ä¼ çŠ¶æ€ï¼‰
        æ¸…ç†æ—§çš„çŠ¶æ€è®°å½•ï¼Œé¿å…é‡å¤å¤„ç†

        Args:
            symbols: å…¨éƒ¨è‚¡ç¥¨åˆ—è¡¨
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            List[str]: éœ€è¦å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨
        """
        try:
            self.logger.info("ğŸ“Š æ£€æŸ¥æ‰©å±•æ•°æ®å®Œæ•´æ€§...")

            # é¦–å…ˆæ¸…ç†æ—§çš„å¾…å¤„ç†çŠ¶æ€ï¼Œé¿å…é‡å¤å¤„ç†
            self.logger.info("ğŸ§¹ æ¸…ç†æ—§çš„æ‰©å±•æ•°æ®åŒæ­¥çŠ¶æ€...")
            cleanup_count = self.db_manager.execute(
                """
                DELETE FROM extended_sync_status 
                WHERE target_date = ? AND status = 'pending'
                """,
                (str(target_date),),
            )
            # execute è¿”å› cursorï¼Œéœ€è¦è·å– rowcount
            affected_rows = (
                cleanup_count.rowcount if hasattr(cleanup_count, "rowcount") else 0
            )
            if affected_rows > 0:
                self.logger.info(f"ğŸ§¹ æ¸…ç†äº† {affected_rows} æ¡æ—§çš„å¾…å¤„ç†çŠ¶æ€")

            # æ£€æŸ¥extended_sync_statusè¡¨ä¸­å·²å®Œæˆçš„è‚¡ç¥¨
            completed_symbols = set()
            completed_status = self.db_manager.fetchall(
                """
                SELECT DISTINCT symbol FROM extended_sync_status 
                WHERE target_date = ? AND status = 'completed'
                """,
                (str(target_date),),
            )
            completed_symbols = set(row["symbol"] for row in completed_status)
            self.logger.info(
                f"ğŸ“‹ ä»åŒæ­¥çŠ¶æ€è¡¨å‘ç°å·²å®Œæˆ: {len(completed_symbols)} åªè‚¡ç¥¨"
            )

            # ç›´æ¥æ£€æŸ¥å®é™…æ•°æ®è¡¨çš„å®Œæ•´æ€§ï¼Œè€Œä¸æ˜¯ä¾èµ–çŠ¶æ€è¡¨
            symbols_needing_processing = []

            if not symbols:
                return []

            # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„æ•°æ®
            placeholders = ",".join(["?" for _ in symbols])

            # 1. æ£€æŸ¥è´¢åŠ¡æ•°æ®ï¼ˆå¹´æŠ¥æ•°æ®ï¼‰
            report_date = f"{target_date.year}-12-31"
            financial_query = f"""
                SELECT DISTINCT symbol FROM financials 
                WHERE symbol IN ({placeholders}) 
                AND report_date = ? 
                AND created_at > datetime('now', '-30 days')
            """
            financial_results = self.db_manager.fetchall(
                financial_query, symbols + [report_date]
            )
            financial_symbols = set(row["symbol"] for row in financial_results)

            # 2. æ£€æŸ¥ä¼°å€¼æ•°æ®ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¼°å€¼æ•°æ®ï¼‰
            valuation_query = f"""
                SELECT DISTINCT symbol FROM valuations 
                WHERE symbol IN ({placeholders})
            """
            valuation_results = self.db_manager.fetchall(valuation_query, symbols)
            valuation_symbols = set(row["symbol"] for row in valuation_results)

            # 3. æ£€æŸ¥æŠ€æœ¯æŒ‡æ ‡ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼‰
            indicator_query = f"""
                SELECT DISTINCT symbol FROM technical_indicators 
                WHERE symbol IN ({placeholders})
            """
            indicator_results = self.db_manager.fetchall(indicator_query, symbols)
            indicator_symbols = set(row["symbol"] for row in indicator_results)

            # ç»Ÿè®¡å®Œæ•´æ€§
            self.logger.info(
                f"ğŸ“Š æ•°æ®å®Œæ•´æ€§: è´¢åŠ¡ {len(financial_symbols)}, ä¼°å€¼ {len(valuation_symbols)}, æŠ€æœ¯æŒ‡æ ‡ {len(indicator_symbols)}"
            )

            # åªæœ‰ç¼ºå°‘ä»»ä½•ä¸€ç§æ•°æ®ä¸”æœªåœ¨åŒæ­¥çŠ¶æ€è¡¨ä¸­æ ‡è®°ä¸ºå·²å®Œæˆçš„è‚¡ç¥¨æ‰éœ€è¦å¤„ç†
            for symbol in symbols:
                # å¦‚æœåœ¨åŒæ­¥çŠ¶æ€è¡¨ä¸­å·²æ ‡è®°ä¸ºå®Œæˆï¼Œè·³è¿‡
                if symbol in completed_symbols:
                    continue

                needs_financial = symbol not in financial_symbols
                needs_valuation = symbol not in valuation_symbols
                needs_indicators = symbol not in indicator_symbols

                # å¦‚æœä»»ä½•ä¸€ç§æ•°æ®ç¼ºå¤±ï¼Œå°±éœ€è¦å¤„ç†è¿™åªè‚¡ç¥¨
                if needs_financial or needs_valuation or needs_indicators:
                    symbols_needing_processing.append(symbol)

            if symbols_needing_processing:
                self.logger.info(
                    f"ğŸ“‹ éœ€è¦å¤„ç†æ‰©å±•æ•°æ®: {len(symbols_needing_processing)} åªè‚¡ç¥¨"
                )

                # æ˜¾ç¤ºè¯¦ç»†çš„ç¼ºå¤±åˆ†å¸ƒ
                missing_financial = len(
                    [
                        s
                        for s in symbols_needing_processing
                        if s not in financial_symbols
                    ]
                )
                missing_valuation = len(
                    [
                        s
                        for s in symbols_needing_processing
                        if s not in valuation_symbols
                    ]
                )
                missing_indicators = len(
                    [
                        s
                        for s in symbols_needing_processing
                        if s not in indicator_symbols
                    ]
                )

                self.logger.info(
                    f"ç¼ºå¤±æ•°æ®åˆ†å¸ƒ: è´¢åŠ¡ {missing_financial}, ä¼°å€¼ {missing_valuation}, æŠ€æœ¯æŒ‡æ ‡ {missing_indicators}"
                )
            else:
                self.logger.info(f"âœ… æ‰€æœ‰è‚¡ç¥¨çš„æ‰©å±•æ•°æ®å·²å®Œæ•´")

            return symbols_needing_processing

        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥æ‰©å±•æ•°æ®å®Œæ•´æ€§å¤±è´¥: {e}")
            import traceback

            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰è‚¡ç¥¨ï¼Œç¡®ä¿ä¸é—æ¼
            return symbols

    def _update_trading_calendar(self, target_date: date) -> Dict[str, Any]:
        """å¢é‡æ›´æ–°äº¤æ˜“æ—¥å†"""
        self.logger.info(f"ğŸ”„ å¼€å§‹äº¤æ˜“æ—¥å†å¢é‡æ›´æ–°ï¼Œç›®æ ‡æ—¥æœŸ: {target_date}")

        # æ£€æŸ¥ç°æœ‰æ•°æ®èŒƒå›´
        existing_range = self.db_manager.fetchone(
            "SELECT MIN(date) as min_date, MAX(date) as max_date, COUNT(*) as count FROM trading_calendar"
        )

        # è®¡ç®—éœ€è¦æ›´æ–°çš„å¹´ä»½
        needed_start_year = target_date.year - 1
        needed_end_year = target_date.year + 1
        years_to_update = list(range(needed_start_year, needed_end_year + 1))

        if existing_range and existing_range["count"] > 0:
            from datetime import datetime

            existing_min = datetime.strptime(
                existing_range["min_date"], "%Y-%m-%d"
            ).date()
            existing_max = datetime.strptime(
                existing_range["max_date"], "%Y-%m-%d"
            ).date()

            # åªæ·»åŠ ç¼ºå¤±çš„å¹´ä»½
            years_to_update = [
                y
                for y in years_to_update
                if y < existing_min.year or y > existing_max.year
            ]

            if not years_to_update:
                return {
                    "status": "skipped",
                    "message": "äº¤æ˜“æ—¥å†å·²æ˜¯æœ€æ–°",
                    "start_year": existing_min.year,
                    "end_year": existing_max.year,
                    "updated_records": 0,
                    "total_records": existing_range["count"],
                }

        self.logger.info(f"éœ€è¦æ›´æ–°å¹´ä»½: {years_to_update}")
        total_inserted = 0

        # è·å–å¹¶æ’å…¥æ•°æ®
        for year in years_to_update:
            start_date = f"{year}-01-01"
            end_date = f"{year}-12-31"

            calendar_data = self.data_source_manager.get_trade_calendar(
                start_date, end_date
            )

            if isinstance(calendar_data, dict) and "data" in calendar_data:
                calendar_data = calendar_data["data"]

            if not calendar_data or not isinstance(calendar_data, list):
                continue

            # æ’å…¥æ•°æ®
            for record in calendar_data:
                self.db_manager.execute(
                    "INSERT OR REPLACE INTO trading_calendar (date, market, is_trading) VALUES (?, ?, ?)",
                    (
                        record.get("trade_date", record.get("date")),
                        "CN",
                        record.get("is_trading", 1),
                    ),
                )
                total_inserted += 1

        # éªŒè¯ç»“æœ
        final_range = self.db_manager.fetchone(
            "SELECT MIN(date) as min_date, MAX(date) as max_date, COUNT(*) as count FROM trading_calendar"
        )

        return {
            "status": "completed",
            "start_year": (
                final_range["min_date"][:4] if final_range else needed_start_year
            ),
            "end_year": final_range["max_date"][:4] if final_range else needed_end_year,
            "updated_records": total_inserted,
            "total_records": final_range["count"] if final_range else 0,
        }

    def _update_stock_list(self) -> Dict[str, Any]:
        """å¢é‡æ›´æ–°è‚¡ç¥¨åˆ—è¡¨"""
        self.logger.info("ğŸ”„ å¼€å§‹è‚¡ç¥¨åˆ—è¡¨å¢é‡æ›´æ–°...")

        # æ£€æŸ¥ç°æœ‰è‚¡ç¥¨
        existing_stats = self.db_manager.fetchone(
            "SELECT COUNT(*) as total_count, COUNT(CASE WHEN status = 'active' THEN 1 END) as active_count FROM stocks"
        )

        total_existing = existing_stats["total_count"] if existing_stats else 0
        active_existing = existing_stats["active_count"] if existing_stats else 0

        # è·å–è‚¡ç¥¨ä¿¡æ¯
        stock_info = self.data_source_manager.get_stock_info()

        # è§£åŒ…åµŒå¥—æ•°æ®
        if isinstance(stock_info, dict) and "data" in stock_info:
            stock_info = stock_info["data"]
            if isinstance(stock_info, dict) and "data" in stock_info:
                stock_info = stock_info["data"]

        if stock_info is None or (hasattr(stock_info, "empty") and stock_info.empty):
            return {
                "status": "completed",
                "total_stocks": total_existing,
                "active_stocks": active_existing,
                "new_stocks": 0,
                "updated_stocks": 0,
            }

        # ç®€åŒ–å¤„ç†ï¼šåªç»Ÿè®¡æ•°é‡
        new_stocks = 0
        total_processed = 0

        if hasattr(stock_info, "iterrows"):  # DataFrame
            total_processed = len(stock_info)
            new_stocks = max(0, total_processed - total_existing)  # ç®€åŒ–ä¼°ç®—
        elif isinstance(stock_info, list):
            total_processed = len(stock_info)
            new_stocks = max(0, total_processed - total_existing)

        return {
            "status": "completed",
            "total_stocks": total_existing + new_stocks,
            "active_stocks": active_existing + new_stocks,
            "new_stocks": new_stocks,
            "updated_stocks": 0,
            "processed_stocks": total_processed,
        }

    def _sync_extended_data(
        self, symbols: List[str], target_date: date, progress_bar=None
    ) -> Dict[str, Any]:
        """å¢é‡åŒæ­¥æ‰©å±•æ•°æ®ï¼ˆè´¢åŠ¡æ•°æ®ã€ä¼°å€¼æ•°æ®ç­‰ï¼‰"""
        import uuid

        session_id = str(uuid.uuid4())
        self.logger.info(f"ğŸ”„ å¼€å§‹æ‰©å±•æ•°æ®åŒæ­¥: {len(symbols)}åªè‚¡ç¥¨")

        result = {
            "financials_count": 0,
            "valuations_count": 0,
            "indicators_count": 0,
            "processed_symbols": 0,
            "failed_symbols": 0,
            "session_id": session_id,
        }

        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„symbolså‚æ•°ï¼Œå› ä¸ºå·²ç»ç»è¿‡_get_extended_data_symbols_to_processè¿‡æ»¤
        self.logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†: {len(symbols)}åªè‚¡ç¥¨")

        if not symbols:
            self.logger.info("âœ… æ²¡æœ‰è‚¡ç¥¨éœ€è¦å¤„ç†")
            if progress_bar:
                progress_bar.update(0)
            return result

        # å¤„ç†æ¯åªè‚¡ç¥¨
        for i, symbol in enumerate(symbols):
            self.logger.debug(f"å¤„ç† {symbol} ({i+1}/{len(symbols)})")

            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™åªè‚¡ç¥¨
            existing_status = self.db_manager.fetchone(
                "SELECT status FROM extended_sync_status WHERE symbol = ? AND target_date = ? AND session_id = ?",
                (symbol, str(target_date), session_id),
            )

            if existing_status and existing_status["status"] == "completed":
                self.logger.debug(f"è·³è¿‡å·²å®Œæˆçš„è‚¡ç¥¨: {symbol}")
                result["processed_symbols"] += 1
                if progress_bar:
                    progress_bar.update(1)
                continue

            # æ ‡è®°å¼€å§‹å¤„ç†
            self.db_manager.execute(
                "INSERT OR REPLACE INTO extended_sync_status (symbol, sync_type, target_date, status, session_id, created_at, updated_at) VALUES (?, ?, ?, ?, ?, datetime('now'), datetime('now'))",
                (symbol, "processing", str(target_date), "processing", session_id),
            )

            # å¤„ç†è´¢åŠ¡æ•°æ®
            financial_data = self.data_source_manager.get_fundamentals(
                symbol, f"{target_date.year}-12-31", "Q4"
            )
            if (
                financial_data
                and isinstance(financial_data, dict)
                and "data" in financial_data
            ):
                # ä½¿ç”¨é€šç”¨æ‰§è¡Œæ–¹æ³•æ’å…¥è´¢åŠ¡æ•°æ®
                self.db_manager.execute(
                    "INSERT OR REPLACE INTO financials (symbol, report_date, report_type, revenue, net_profit, source, created_at) VALUES (?, ?, ?, ?, ?, ?, datetime('now'))",
                    (
                        symbol,
                        f"{target_date.year}-12-31",
                        "Q4",
                        financial_data["data"].get("revenue", 0),
                        financial_data["data"].get("net_profit", 0),
                        "akshare",
                    ),
                )
                result["financials_count"] += 1

            # å¤„ç†ä¼°å€¼æ•°æ®
            valuation_data = self.data_source_manager.get_valuation_data(
                symbol, str(target_date)
            )
            if (
                valuation_data
                and isinstance(valuation_data, dict)
                and "data" in valuation_data
            ):
                # ä½¿ç”¨é€šç”¨æ‰§è¡Œæ–¹æ³•æ’å…¥ä¼°å€¼æ•°æ®
                self.db_manager.execute(
                    "INSERT OR REPLACE INTO valuations (symbol, date, pe_ratio, pb_ratio, source, created_at) VALUES (?, ?, ?, ?, ?, datetime('now'))",
                    (
                        symbol,
                        str(target_date),
                        valuation_data["data"].get("pe_ratio", 0),
                        valuation_data["data"].get("pb_ratio", 0),
                        "akshare",
                    ),
                )
                result["valuations_count"] += 1

            # å¤„ç†æŠ€æœ¯æŒ‡æ ‡ - ç®€åŒ–å¤„ç†
            # ä½¿ç”¨è™šæ‹Ÿæ•°æ®æ’å…¥æŠ€æœ¯æŒ‡æ ‡
            self.db_manager.execute(
                "INSERT OR REPLACE INTO technical_indicators (symbol, date, ma5, ma10, calculated_at) VALUES (?, ?, ?, ?, datetime('now'))",
                (symbol, str(target_date), 0.0, 0.0),
            )
            result["indicators_count"] += 1

            # æ ‡è®°å®Œæˆå¤„ç†
            self.db_manager.execute(
                "UPDATE extended_sync_status SET status = 'completed', updated_at = datetime('now') WHERE symbol = ? AND target_date = ? AND session_id = ?",
                (symbol, str(target_date), session_id),
            )

            result["processed_symbols"] += 1
            if progress_bar:
                progress_bar.update(1)

        return result

    def _auto_fix_gaps(self, gap_result: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªåŠ¨ä¿®å¤ç¼ºå£"""
        self.logger.info("å¼€å§‹è‡ªåŠ¨ä¿®å¤ç¼ºå£")

        fix_result = {
            "total_gaps": gap_result["summary"]["total_gaps"],
            "attempted_fixes": 0,
            "successful_fixes": 0,
            "failed_fixes": 0,
            "fix_details": [],
        }

        # è·å–ç¼ºå£è¯¦æƒ…
        gaps_by_symbol = gap_result.get("gaps_by_symbol", {})

        if not gaps_by_symbol:
            self.logger.info("æ²¡æœ‰å‘ç°ç¼ºå£ï¼Œæ— éœ€ä¿®å¤")
            return fix_result

        # é™åˆ¶ä¿®å¤æ•°é‡ï¼Œé¿å…è¿‡é•¿æ—¶é—´
        max_fixes = 20
        fixes_attempted = 0

        for symbol, symbol_gaps in gaps_by_symbol.items():
            if fixes_attempted >= max_fixes:
                self.logger.info(f"å·²è¾¾åˆ°æœ€å¤§ä¿®å¤æ•°é‡é™åˆ¶: {max_fixes}")
                break

            for gap in symbol_gaps.get("gaps", []):
                if fixes_attempted >= max_fixes:
                    break

                try:
                    gap_start = gap.get("gap_start")
                    gap_end = gap.get("gap_end")
                    frequency = gap.get("frequency", "1d")

                    if not gap_start or not gap_end:
                        continue

                    fix_result["attempted_fixes"] += 1
                    fixes_attempted += 1

                    self.logger.info(f"ä¿®å¤ç¼ºå£: {symbol} {gap_start} åˆ° {gap_end}")

                    # å°è¯•ä»æ•°æ®æºè·å–ç¼ºå£æœŸé—´çš„æ•°æ®
                    if frequency == "1d":
                        # è·å–æ—¥çº¿æ•°æ®å¡«è¡¥ç¼ºå£
                        daily_data = self.data_source_manager.get_daily_data(
                            symbol, gap_start, gap_end
                        )

                        if isinstance(daily_data, dict) and "data" in daily_data:
                            daily_data = daily_data["data"]

                        # æ£€æŸ¥è·å–åˆ°çš„æ•°æ®
                        if daily_data is not None and hasattr(daily_data, "__len__"):
                            # å¦‚æœæ˜¯DataFrameæˆ–åˆ—è¡¨ï¼Œå¤„ç†æ•°æ®
                            records_inserted = 0

                            if hasattr(daily_data, "iterrows"):
                                # pandas DataFrame
                                for _, row in daily_data.iterrows():
                                    try:
                                        # ä½¿ç”¨æ•°æ®å¤„ç†å¼•æ“æ’å…¥æ•°æ®
                                        processed_result = (
                                            self.processing_engine.process_symbol_data(
                                                symbol,
                                                str(gap_start),
                                                str(gap_end),
                                                frequency,
                                            )
                                        )
                                        records_inserted += processed_result.get(
                                            "records", 0
                                        )
                                        break  # å¤„ç†å¼•æ“ä¼šå¤„ç†æ•´ä¸ªæ—¥æœŸèŒƒå›´
                                    except Exception as e:
                                        self.logger.warning(
                                            f"æ’å…¥ç¼ºå£æ•°æ®å¤±è´¥ {symbol}: {e}"
                                        )

                            if records_inserted > 0:
                                fix_result["successful_fixes"] += 1
                                fix_result["fix_details"].append(
                                    {
                                        "symbol": symbol,
                                        "gap_start": gap_start,
                                        "gap_end": gap_end,
                                        "records_inserted": records_inserted,
                                        "status": "success",
                                    }
                                )
                                self.logger.info(
                                    f"ç¼ºå£ä¿®å¤æˆåŠŸ: {symbol} æ’å…¥ {records_inserted} æ¡è®°å½•"
                                )
                            else:
                                fix_result["failed_fixes"] += 1
                                fix_result["fix_details"].append(
                                    {
                                        "symbol": symbol,
                                        "gap_start": gap_start,
                                        "gap_end": gap_end,
                                        "status": "failed",
                                        "reason": "æ— æ•°æ®å¯æ’å…¥",
                                    }
                                )
                        else:
                            fix_result["failed_fixes"] += 1
                            fix_result["fix_details"].append(
                                {
                                    "symbol": symbol,
                                    "gap_start": gap_start,
                                    "gap_end": gap_end,
                                    "status": "failed",
                                    "reason": "æ•°æ®æºæ— æ•°æ®",
                                }
                            )
                    else:
                        # å…¶ä»–é¢‘ç‡çš„ç¼ºå£ä¿®å¤æš‚ä¸å®ç°
                        fix_result["failed_fixes"] += 1
                        fix_result["fix_details"].append(
                            {
                                "symbol": symbol,
                                "gap_start": gap_start,
                                "gap_end": gap_end,
                                "status": "failed",
                                "reason": f"ä¸æ”¯æŒé¢‘ç‡ {frequency}",
                            }
                        )

                except Exception as e:
                    fix_result["failed_fixes"] += 1
                    fix_result["fix_details"].append(
                        {
                            "symbol": symbol,
                            "gap_start": gap.get("gap_start"),
                            "gap_end": gap.get("gap_end"),
                            "status": "error",
                            "reason": str(e),
                        }
                    )
                    self.logger.error(f"ä¿®å¤ç¼ºå£æ—¶å‘ç”Ÿé”™è¯¯ {symbol}: {e}")

        self.logger.info(
            f"ç¼ºå£ä¿®å¤å®Œæˆ: æ€»ç¼ºå£={fix_result['total_gaps']}, å°è¯•ä¿®å¤={fix_result['attempted_fixes']}, æˆåŠŸ={fix_result['successful_fixes']}, å¤±è´¥={fix_result['failed_fixes']}"
        )
        return fix_result

    def generate_sync_report(self, full_result: Dict[str, Any]) -> str:
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        try:
            report_lines = []

            # æŠ¥å‘Šå¤´éƒ¨
            report_lines.append("=" * 60)
            report_lines.append("æ•°æ®åŒæ­¥æŠ¥å‘Š")
            report_lines.append("=" * 60)
            report_lines.append(f"åŒæ­¥æ—¶é—´: {full_result.get('start_time', '')}")
            report_lines.append(f"ç›®æ ‡æ—¥æœŸ: {full_result.get('target_date', '')}")
            report_lines.append(
                f"æ€»è€—æ—¶: {full_result.get('duration_seconds', 0):.2f} ç§’"
            )
            report_lines.append("")

            # é˜¶æ®µæ±‡æ€»
            summary = full_result.get("summary", {})
            report_lines.append("é˜¶æ®µæ±‡æ€»:")
            report_lines.append(f"  æ€»é˜¶æ®µæ•°: {summary.get('total_phases', 0)}")
            report_lines.append(f"  æˆåŠŸé˜¶æ®µ: {summary.get('successful_phases', 0)}")
            report_lines.append(f"  å¤±è´¥é˜¶æ®µ: {summary.get('failed_phases', 0)}")
            report_lines.append("")

            # å„é˜¶æ®µè¯¦æƒ…
            phases = full_result.get("phases", {})

            # å¢é‡åŒæ­¥
            if "incremental_sync" in phases:
                phase = phases["incremental_sync"]
                report_lines.append("å¢é‡åŒæ­¥:")
                report_lines.append(f"  çŠ¶æ€: {phase['status']}")

                if phase["status"] == "completed" and "result" in phase:
                    result = phase["result"]
                    report_lines.append(f"  æ€»è‚¡ç¥¨æ•°: {result.get('total_symbols', 0)}")
                    report_lines.append(f"  æˆåŠŸæ•°é‡: {result.get('success_count', 0)}")
                    report_lines.append(f"  é”™è¯¯æ•°é‡: {result.get('error_count', 0)}")
                    report_lines.append(f"  è·³è¿‡æ•°é‡: {result.get('skipped_count', 0)}")
                elif "error" in phase:
                    report_lines.append(f"  é”™è¯¯: {phase['error']}")

                report_lines.append("")

            return "\n".join(report_lines)

        except Exception as e:
            self._log_error("generate_sync_report", e)
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"

    def _safe_get_attribute(self, obj, key: str, default=None):
        """å®‰å…¨è·å–å¯¹è±¡å±æ€§ï¼Œå…¼å®¹dictå’Œsqlite3.Row"""
        if obj is None:
            return default

        try:
            if hasattr(obj, "get"):
                return obj.get(key, default)
            elif hasattr(obj, "__getitem__"):
                return obj[key]
        except (KeyError, IndexError, TypeError):
            return default

        return default

    def _calculate_technical_indicators(
        self,
        symbol: str,
        target_date: date,
        indicator_calculator,
        existing_indicators: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        è®¡ç®—å•ä¸ªè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            target_date: ç›®æ ‡æ—¥æœŸ
            indicator_calculator: æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å™¨
            existing_indicators: å·²å­˜åœ¨çš„æŒ‡æ ‡æ•°æ®

        Returns:
            Dict[str, Any]: è®¡ç®—ç»“æœ {"success": bool, "indicators": dict, "message": str}
        """
        from datetime import datetime, timedelta

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        daily_update_threshold = timedelta(days=1)
        if existing_indicators:
            try:
                # å®‰å…¨è·å– last_update å­—æ®µï¼Œå…¼å®¹ dict å’Œ sqlite3.Row
                last_update_value = self._safe_get_attribute(
                    existing_indicators, "last_update"
                )

                if last_update_value:
                    last_update = datetime.fromisoformat(
                        last_update_value.replace("Z", "+00:00")
                        if last_update_value.endswith("Z")
                        else last_update_value
                    )
                    if datetime.now() - last_update < daily_update_threshold:
                        return {
                            "success": False,
                            "message": "recently_updated",
                            "indicators": None,
                        }
            except Exception:
                pass  # å¦‚æœè§£ææ—¶é—´å¤±è´¥ï¼Œç»§ç»­è®¡ç®—

        # è·å–å†å²æ•°æ®
        start_date = target_date - timedelta(days=100)
        try:
            historical_data = self.data_source_manager.get_daily_data(
                symbol, start_date, target_date
            )
        except Exception as e:
            return {
                "success": False,
                "message": f"è·å–å†å²æ•°æ®å¤±è´¥: {e}",
                "indicators": None,
            }

        # å¤„ç†å†å²æ•°æ®æ ¼å¼
        processed_data = self._process_historical_data(historical_data)
        if not processed_data:
            return {
                "success": False,
                "message": "å†å²æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯",
                "indicators": None,
            }

        # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
        data_length = self._get_data_length(processed_data)
        if data_length < 20:
            return {
                "success": False,
                "message": f"å†å²æ•°æ®ä¸è¶³({data_length}æ¡)",
                "indicators": None,
            }

        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        try:
            # ä¸´æ—¶é™ä½æ—¥å¿—çº§åˆ«ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡
            indicators_logger = logging.getLogger(
                "simtradedata.preprocessor.indicators"
            )
            original_level = indicators_logger.level
            indicators_logger.setLevel(logging.ERROR)

            try:
                indicators_data = indicator_calculator.calculate_indicators(
                    processed_data, symbol
                )
            finally:
                indicators_logger.setLevel(original_level)

            if not indicators_data or not isinstance(indicators_data, dict):
                return {
                    "success": False,
                    "message": "æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç»“æœä¸ºç©º",
                    "indicators": None,
                }

            # æå–æœ€æ–°æŒ‡æ ‡å€¼
            latest_indicators = self._extract_latest_indicators(indicators_data)
            if not latest_indicators:
                return {
                    "success": False,
                    "message": "æ— æ³•æå–æœ€æ–°æŒ‡æ ‡å€¼",
                    "indicators": None,
                }

            return {
                "success": True,
                "message": "è®¡ç®—æˆåŠŸ",
                "indicators": latest_indicators,
            }

        except Exception as e:
            return {"success": False, "message": f"è®¡ç®—å¼‚å¸¸: {e}", "indicators": None}

    def _process_historical_data(self, historical_data) -> Any:
        """å¤„ç†å†å²æ•°æ®æ ¼å¼"""
        if historical_data is None:
            return None

        if isinstance(historical_data, dict) and "data" in historical_data:
            return historical_data["data"]

        return historical_data

    def _get_data_length(self, data) -> int:
        """è·å–æ•°æ®é•¿åº¦"""
        if hasattr(data, "__len__"):
            return len(data)
        elif hasattr(data, "shape"):
            return data.shape[0]
        return 0

    def _extract_latest_indicators(
        self, indicators_data: Dict[str, Any]
    ) -> Dict[str, float]:
        """æå–æœ€æ–°çš„æŒ‡æ ‡å€¼"""
        latest_indicators = {}
        for indicator_name, values in indicators_data.items():
            if isinstance(values, (list, tuple)) and len(values) > 0:
                latest_indicators[indicator_name] = values[-1]
            elif isinstance(values, (int, float)):
                latest_indicators[indicator_name] = values
        return latest_indicators

    def _initialize_extended_sync_status(
        self, symbols: List[str], target_date: date, session_id: str
    ):
        """åˆå§‹åŒ–æ‰©å±•æ•°æ®åŒæ­¥çŠ¶æ€è®°å½• - åªä¸ºä¸å­˜åœ¨çš„è®°å½•åˆ›å»ºçŠ¶æ€"""
        try:
            sync_types = ["financials", "valuations", "indicators"]

            for symbol in symbols:
                for sync_type in sync_types:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®°å½•
                    existing = self.db_manager.fetchone(
                        """
                        SELECT 1 FROM extended_sync_status 
                        WHERE symbol = ? AND sync_type = ? AND target_date = ?
                        """,
                        (symbol, sync_type, str(target_date)),
                    )

                    # åªæœ‰ä¸å­˜åœ¨æ—¶æ‰æ’å…¥æ–°è®°å½•
                    if not existing:
                        self.db_manager.execute(
                            """
                            INSERT INTO extended_sync_status 
                            (symbol, sync_type, target_date, status, phase, session_id, created_at, updated_at)
                            VALUES (?, ?, ?, 'pending', 'extended_data', ?, datetime('now'), datetime('now'))
                            """,
                            (symbol, sync_type, str(target_date), session_id),
                        )

            self.logger.debug(
                f"åˆå§‹åŒ–æ‰©å±•æ•°æ®åŒæ­¥çŠ¶æ€: {len(symbols)}åªè‚¡ç¥¨ x 3ç§ç±»å‹ (ä»…æ–°å¢)"
            )

        except Exception as e:
            self.logger.warning(f"åˆå§‹åŒ–æ‰©å±•æ•°æ®åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")

    def _update_sync_status(
        self,
        symbol: str,
        sync_type: str,
        target_date: str,
        status: str,
        session_id: str,
        records_count: int = 0,
    ):
        """æ›´æ–°å•ä¸ªè‚¡ç¥¨çš„åŒæ­¥çŠ¶æ€"""
        try:
            # ç¡®ä¿æ­£ç¡®æ›´æ–°æ‰€æœ‰å¿…è¦å­—æ®µ
            self.db_manager.execute(
                """
                INSERT OR REPLACE INTO extended_sync_status 
                (symbol, sync_type, target_date, status, last_updated, phase, session_id, records_count, created_at, updated_at)
                VALUES (?, ?, ?, ?, datetime('now'), 'extended_data', ?, ?, 
                        COALESCE((SELECT created_at FROM extended_sync_status WHERE symbol=? AND sync_type=? AND target_date=?), datetime('now')), 
                        datetime('now'))
                """,
                (
                    symbol,
                    sync_type,
                    target_date,
                    status,
                    session_id,
                    records_count,
                    symbol,
                    sync_type,
                    target_date,
                ),
            )
            self.logger.debug(f"æ›´æ–°åŒæ­¥çŠ¶æ€: {symbol}-{sync_type} -> {status}")
        except Exception as e:
            self.logger.warning(f"æ›´æ–°åŒæ­¥çŠ¶æ€å¤±è´¥ {symbol}-{sync_type}: {e}")
            import traceback

            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    def _get_sync_summary(self, target_date: str, session_id: str) -> Dict[str, Any]:
        """è·å–åŒæ­¥æ±‡æ€»ä¿¡æ¯"""
        try:
            summary_query = """
                SELECT sync_type, status, COUNT(*) as count, SUM(records_count) as total_records
                FROM extended_sync_status 
                WHERE target_date = ? AND session_id = ?
                GROUP BY sync_type, status
                ORDER BY sync_type, status
            """

            summary_results = self.db_manager.fetchall(
                summary_query, (target_date, session_id)
            )

            result = {
                "financials_count": 0,
                "valuations_count": 0,
                "indicators_count": 0,
                "processed_symbols": 0,
                "failed_symbols": 0,
                "skipped_symbols": 0,
                "errors": [],
                "session_id": session_id,
            }

            for row in summary_results:
                sync_type = row["sync_type"]
                status = row["status"]
                count = row["count"]
                records = row["total_records"] or 0

                if sync_type == "financials" and status == "completed":
                    result["financials_count"] = records
                elif sync_type == "valuations" and status == "completed":
                    result["valuations_count"] = records
                elif sync_type == "indicators" and status == "completed":
                    result["indicators_count"] = records

                if status == "completed":
                    result["processed_symbols"] += count
                elif status == "failed":
                    result["failed_symbols"] += count
                elif status == "skipped":
                    result["skipped_symbols"] += count

            return result

        except Exception as e:
            self.logger.warning(f"è·å–åŒæ­¥æ±‡æ€»å¤±è´¥: {e}")
            return {"error": str(e), "session_id": session_id}

    def _get_sync_status_for_type(
        self, symbol: str, sync_type: str, target_date: str
    ) -> str:
        """è·å–ç‰¹å®šè‚¡ç¥¨å’Œæ•°æ®ç±»å‹çš„åŒæ­¥çŠ¶æ€"""
        try:
            result = self.db_manager.fetchone(
                """
                SELECT status FROM extended_sync_status 
                WHERE symbol = ? AND sync_type = ? AND target_date = ?
                """,
                (symbol, sync_type, target_date),
            )
            return result["status"] if result else "pending"
        except Exception as e:
            self.logger.debug(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥ {symbol}-{sync_type}: {e}")
            return "pending"

    def _filter_symbols_needing_extended_data(
        self, symbols: List[str], target_date: date
    ) -> List[str]:
        """
        æ™ºèƒ½è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦å¤„ç†æ‰©å±•æ•°æ®çš„è‚¡ç¥¨
        è·³è¿‡å·²æœ‰å®Œæ•´æ•°æ®çš„è‚¡ç¥¨ï¼Œå¤§å¹…å‡å°‘å¤„ç†é‡
        """
        try:
            symbols_needing_processing = []

            # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„æ•°æ®
            if not symbols:
                return []

            # æ£€æŸ¥è´¢åŠ¡æ•°æ®ï¼ˆå¹´æŠ¥æ•°æ®ï¼Œé€šå¸¸ä¸éœ€è¦é¢‘ç¹æ›´æ–°ï¼‰
            report_date = f"{target_date.year}-12-31"
            financial_symbols = set()

            if len(symbols) > 0:
                placeholders = ",".join(["?" for _ in symbols])
                financial_query = f"""
                    SELECT DISTINCT symbol FROM financials 
                    WHERE symbol IN ({placeholders}) 
                    AND report_date = ? 
                    AND created_at > datetime('now', '-30 days')
                """
                financial_results = self.db_manager.fetchall(
                    financial_query, symbols + [report_date]
                )
                financial_symbols = set(row["symbol"] for row in financial_results)

            # æ£€æŸ¥ä¼°å€¼æ•°æ®ï¼ˆæ—¥æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å½“æ—¥æ•°æ®ï¼‰
            valuation_symbols = set()
            if len(symbols) > 0:
                valuation_query = f"""
                    SELECT DISTINCT symbol FROM valuations 
                    WHERE symbol IN ({placeholders}) 
                    AND date = ? 
                    AND created_at > datetime('now', '-1 days')
                """
                valuation_results = self.db_manager.fetchall(
                    valuation_query, symbols + [str(target_date)]
                )
                valuation_symbols = set(row["symbol"] for row in valuation_results)

            # æ£€æŸ¥æŠ€æœ¯æŒ‡æ ‡ï¼ˆæ—¥æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å½“æ—¥æ•°æ®ï¼‰
            indicator_symbols = set()
            if len(symbols) > 0:
                indicator_query = f"""
                    SELECT DISTINCT symbol FROM technical_indicators 
                    WHERE symbol IN ({placeholders}) 
                    AND date = ? 
                    AND calculated_at > datetime('now', '-1 days')
                """
                indicator_results = self.db_manager.fetchall(
                    indicator_query, symbols + [str(target_date)]
                )
                indicator_symbols = set(row["symbol"] for row in indicator_results)

            # åªå¤„ç†ç¼ºå°‘æ•°æ®çš„è‚¡ç¥¨
            for symbol in symbols:
                needs_financial = symbol not in financial_symbols
                needs_valuation = symbol not in valuation_symbols
                needs_indicators = symbol not in indicator_symbols

                # å¦‚æœä»»ä½•ä¸€ç§æ•°æ®ç¼ºå¤±ï¼Œå°±éœ€è¦å¤„ç†è¿™åªè‚¡ç¥¨
                if needs_financial or needs_valuation or needs_indicators:
                    symbols_needing_processing.append(symbol)

            self.logger.info(
                f"ğŸ“Š æ•°æ®å®Œæ•´æ€§æ£€æŸ¥: "
                f"è´¢åŠ¡æ•°æ®å®Œæ•´ {len(financial_symbols)}åª, "
                f"ä¼°å€¼æ•°æ®å®Œæ•´ {len(valuation_symbols)}åª, "
                f"æŠ€æœ¯æŒ‡æ ‡å®Œæ•´ {len(indicator_symbols)}åª"
            )

            return symbols_needing_processing

        except Exception as e:
            self.logger.warning(f"è¿‡æ»¤æ‰©å±•æ•°æ®è‚¡ç¥¨å¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰è‚¡ç¥¨ï¼Œç¡®ä¿ä¸é—æ¼
            return symbols

    def _prioritize_symbols_for_processing(self, symbols: List[str]) -> List[str]:
        """
        ä¸ºæ‰©å±•æ•°æ®å¤„ç†ä¼˜å…ˆæ’åºè‚¡ç¥¨
        ä¼˜å…ˆå¤„ç†æ´»è·ƒçš„å¤§å¸‚å€¼è‚¡ç¥¨
        """
        try:
            if not symbols:
                return []

            # æŸ¥è¯¢è‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯å’Œæœ€è¿‘äº¤æ˜“æ´»è·ƒåº¦
            placeholders = ",".join(["?" for _ in symbols])
            priority_query = f"""
                SELECT s.symbol, s.name, s.market,
                       COALESCE(s.total_shares, 0) as market_cap_proxy,
                       COUNT(md.symbol) as recent_trading_days
                FROM stocks s
                LEFT JOIN market_data md ON s.symbol = md.symbol 
                    AND md.date > date('now', '-30 days') 
                    AND md.frequency = '1d'
                WHERE s.symbol IN ({placeholders})
                    AND s.status = 'active'
                GROUP BY s.symbol, s.name, s.market, s.total_shares
                ORDER BY 
                    recent_trading_days DESC,  -- æœ€è¿‘äº¤æ˜“æ´»è·ƒ
                    market_cap_proxy DESC,     -- å¸‚å€¼å¤§çš„ä¼˜å…ˆ
                    s.symbol ASC               -- ä»£ç æ’åºä¿è¯ç¨³å®šæ€§
            """

            priority_results = self.db_manager.fetchall(priority_query, symbols)

            if priority_results:
                prioritized_symbols = [row["symbol"] for row in priority_results]
                self.logger.debug(
                    f"è‚¡ç¥¨ä¼˜å…ˆçº§æ’åºå®Œæˆ: å‰5åª {prioritized_symbols[:5]}"
                )
                return prioritized_symbols
            else:
                # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›åŸå§‹é¡ºåº
                return symbols

        except Exception as e:
            self.logger.warning(f"è‚¡ç¥¨ä¼˜å…ˆçº§æ’åºå¤±è´¥: {e}")
            return symbols
